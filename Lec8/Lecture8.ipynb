{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 8: Statistical Analysis with `scipy.stats`\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The `scipy.stats` module in SciPy is a cornerstone for statistical analysis in Python, offering a broad spectrum of functions for probability distributions, statistical tests, and descriptive statistics. This comprehensive lecture note will delve into the functionalities provided by `scipy.stats`, illustrated with detailed examples to facilitate a deeper understanding of statistical analysis in scientific computing.\n",
        "\n",
        "## 1. Understanding Probability Distributions\n",
        "\n",
        "### 1.1 Continuous Distributions\n",
        "\n",
        "- **Normal Distribution**: Central to many statistical analyses, described by its mean (μ) and standard deviation (σ).\n",
        "\n",
        "```python\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Parameters\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "mu, sigma = 0, 1\n",
        "\n",
        "# Points\n",
        "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
        "\n",
        "# Plot\n",
        "plt.plot(x, norm.pdf(x, mu, sigma))\n",
        "plt.plot(x, norm.cdf(x, mu, sigma))\n",
        "plt.title('Normal Distribution')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('PDF')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# PDF and CDF\n",
        "pdf_val = norm.pdf(0, mu, sigma)\n",
        "cdf_val = norm.cdf(0, mu, sigma)\n",
        "\n",
        "print(f\"PDF at x=0: {pdf_val}\")\n",
        "print(f\"CDF at x=0: {cdf_val}\")\n",
        "```\n",
        "\n",
        "- **Exponential Distribution**: Models the time until an event occurs, with rate parameter λ.\n",
        "\n",
        "```python\n",
        "from scipy.stats import expon\n",
        "\n",
        "lambda_inv = 1  # Mean of exponential distribution, λ^-1\n",
        "exp_dist = expon(scale=lambda_inv)\n",
        "\n",
        "print(f\"PDF at x=1: {exp_dist.pdf(1)}\")\n",
        "```\n",
        "\n",
        "### 1.2 Discrete Distributions\n",
        "\n",
        "- **Binomial Distribution**: Describes the number of successes in n independent Bernoulli trials, with success probability p.\n",
        "\n",
        "```python\n",
        "from scipy.stats import binom\n",
        "\n",
        "n, p = 10, 0.5  # 10 trials, success probability 0.5\n",
        "binom_dist = binom(n, p)\n",
        "\n",
        "print(f\"PMF for 5 successes: {binom_dist.pmf(5)}\")\n",
        "```\n",
        "\n",
        "## 2. Descriptive Statistics\n",
        "\n",
        "`scipy.stats` offers functions to describe and summarize the central tendency, dispersion, and shape of datasets.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "data = np.random.normal(mu, sigma, 1000)\n",
        "\n",
        "# Central tendency\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "\n",
        "# Dispersion\n",
        "std_dev = np.std(data)\n",
        "variance = np.var(data)\n",
        "\n",
        "# Shape\n",
        "skewness = stats.skew(data)\n",
        "kurtosis = stats.kurtosis(data)\n",
        "\n",
        "print(f\"Mean: {mean}, Median: {median}, Std Dev: {std_dev}, Variance: {variance}\")\n",
        "print(f\"Skewness: {skewness}, Kurtosis: {kurtosis}\")\n",
        "\n",
        "# or\n",
        "datastats = stats.describe(data)\n",
        "print(datastats)\n",
        "\n",
        "```\n",
        "\n",
        "## 3. Hypothesis Testing\n",
        "\n",
        "Hypothesis tests are critical for making inferences about populations from sample data. `scipy.stats` includes functions for conducting various statistical tests.\n",
        "\n",
        "### 3.1 T-tests\n",
        "\n",
        "- **One-sample T-test**: Tests if the mean of a sample differs from a known value.\n",
        "\n",
        "```python\n",
        "# Testing if sample mean differs from true mean of 0\n",
        "mu, sigma = 0, 1\n",
        "data = np.random.normal(mu, sigma, 1000)\n",
        "t_stat, p_val = stats.ttest_1samp(data, 0)\n",
        "\n",
        "print(f\"One-sample T-test: T-stat={t_stat}, P-value={p_val}\")\n",
        "```\n",
        "\n",
        "### 3.2 ANOVA (Analysis of Variance)\n",
        "\n",
        "- **One-way ANOVA**: Tests if there are statistically significant differences between the means of three or more independent groups.\n",
        "\n",
        "```python\n",
        "data1 = np.random.normal(mu, sigma, 100)\n",
        "data2 = np.random.normal(mu, sigma, 100)\n",
        "data3 = np.random.normal(mu, sigma, 100)\n",
        "\n",
        "f_val, p_val = stats.f_oneway(data1, data2, data3)\n",
        "\n",
        "print(f\"ANOVA: F-statistic={f_val}, P-value={p_val}\")\n",
        "```\n",
        "\n",
        "## 4. Correlation and Regression\n",
        "\n",
        "Correlation measures the relationship between two variables, while regression models the dependence of a variable on one or more other variables.\n",
        "\n",
        "### 4.1 Pearson Correlation Coefficient\n",
        "\n",
        "```python\n",
        "x = np.random.random(100)\n",
        "y = 2*x + np.random.normal(0, 1, 100)\n",
        "\n",
        "corr_coef, p_val = stats.pearsonr(x, y)\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {corr_coef}, P-value: {p_val}\")\n",
        "```\n",
        "\n",
        "### 4.2 Linear Regression\n",
        "\n",
        "```python\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
        "\n",
        "print(f\"Linear Regression: Slope={slope}, Intercept={intercept}, R-squared={r_value**2}\")\n",
        "```\n",
        "\n",
        "## 5 Testing normality\n",
        "Testing whether a dataset follows a normal distribution is a common task in statistical analysis. The `scipy.stats` module provides several tests for this purpose, including the Shapiro-Wilk test. Such tests evaluate the hypothesis that a dataset comes from a normally distributed population.\n",
        "\n",
        "### Shapiro-Wilk Test\n",
        "\n",
        "The Shapiro-Wilk test is widely used for testing normality due to its good power properties as compared to other tests. It works well for small sample sizes (< 50 samples), but can also be applied to larger datasets.\n",
        "\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Generate sample data\n",
        "data = np.random.normal(loc=0, scale=1, size=100)\n",
        "\n",
        "# Perform the Shapiro-Wilk test for normality\n",
        "shapiro_test_stat, shapiro_p_value = stats.shapiro(data)\n",
        "\n",
        "print(\"Shapiro-Wilk Test Statistic:\", shapiro_test_stat)\n",
        "print(\"Shapiro-Wilk Test P-Value:\", shapiro_p_value)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if shapiro_p_value > alpha:\n",
        "    print(\"Sample looks Gaussian (fail to reject H0)\")\n",
        "else:\n",
        "    print(\"Sample does not look Gaussian (reject H0)\")\n",
        "```\n",
        "\n",
        "### Visual Inspection\n",
        "\n",
        "In addition to statistical tests, visual inspection with QQ-plots (quantile-quantile plots) and histograms can be helpful to assess normality.\n",
        "\n",
        "#### QQ-Plot Example:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Generate QQ plot\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(data, plot=plt)\n",
        "\n",
        "plt.title(\"QQ Plot\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### Histogram Example:\n",
        "\n",
        "```python\n",
        "# Plot histogram\n",
        "plt.hist(data, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
        "plt.title(\"Histogram of the Data\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## 6 Box-Cox Transformation\n",
        "The Box-Cox transformation is a statistical technique used to stabilize variance and make the data more closely resemble a normal distribution.\n",
        "\n",
        "Many statistical methods and techniques assume that the data follows a normal distribution. However, real-world data often deviates from this assumption. The Box-Cox transformation is a powerful method for transforming non-normal dependent variables into a normal shape.\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "The Box-Cox transformation is defined as:\n",
        "\n",
        "\\[\n",
        "y(\\lambda) = \\begin{cases}\n",
        "\\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\n",
        "\\log(y) & \\text{if } \\lambda = 0\n",
        "\\end{cases}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\(y\\) is the response variable that needs to be transformed.\n",
        "- \\(\\lambda\\) is the transformation parameter that varies to find the best approximation to a normal distribution.\n",
        "\n",
        "### Determining the Best \\(\\lambda\\)\n",
        "\n",
        "The value of \\(\\lambda\\) that best normalizes the data is determined through maximum likelihood estimation.\n",
        "\n",
        "### Implementation in Python\n",
        "\n",
        "The `scipy.stats` module provides the `boxcox` function, which automatically finds the optimal \\(\\lambda\\) value and applies the Box-Cox transformation.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate non-normal data\n",
        "data = np.random.exponential(size=1000)\n",
        "\n",
        "# Apply Box-Cox Transformation\n",
        "transformed_data, best_lambda = stats.boxcox(data)\n",
        "\n",
        "print(f\"Optimal λ value: {best_lambda}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Original Data\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data, bins=30, alpha=0.7, color='blue')\n",
        "plt.title(\"Original Data\")\n",
        "\n",
        "# Transformed Data\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(transformed_data, bins=30, alpha=0.7, color='green')\n",
        "plt.title(\"Box-Cox Transformed Data\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Applications\n",
        "\n",
        "- **Improving Linear Model Fit**: The Box-Cox transformation can be applied to the dependent variable in regression analyses to meet the normality assumption.\n",
        "- **Variance Stabilization**: It is useful for stabilizing the variance of a dataset, as many statistical techniques assume homoscedasticity (constant variance).\n",
        "- **Data Preprocessing**: In machine learning, transforming features to more closely follow a Gaussian distribution can improve the performance of models.\n",
        "\n",
        "### Considerations\n",
        "\n",
        "- The Box-Cox transformation requires all data to be positive. If the dataset contains zero or negative values, a constant may be added to all values to make them strictly positive before applying the transformation.\n",
        "- It's important to apply the same transformation to new data before using a model that was trained on transformed data.\n",
        "\n",
        "## 6. Practical Applications\n",
        "\n",
        "- Use `scipy.stats` to print out the statistics of the GPP column in Wcr_GPPdaily.csv.\n",
        "- Apply linregress for a linear regression model between `GPP_NT_VUT_REF` and `SW_IN_F`\n",
        "\n",
        "# **Homework 4**: Building a Linear Regression Model with SciPy Optimization\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this homework assignment, you will apply the concepts of linear regression and optimization to a real-world dataset, `Wcr_GPPdaily.csv`, which contains daily measurements of Gross Primary Productivity (GPP) along with environmental variables. Your task is to build a linear regression model to predict GPP based on soil water content (SW), vapor pressure deficit (VPD), and air temperature (TA), using SciPy's optimization functions.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. **Data Exploration**: Familiarize yourself with the dataset, specifically focusing on GPP, SW, VPD, and TA variables.\n",
        "2. **Preprocessing**: Prepare the data for modeling, ensuring it is clean and appropriately formatted.\n",
        "3. **Model Building**: Construct a linear regression model where GPP is a function of SW, VPD, and TA.\n",
        "4. **Optimization**: Use SciPy's optimization functions to find the best parameters for your linear regression model.\n",
        "5. **Evaluation**: Assess the performance of your model.\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "-  GPP: `GPP_NT_VUT_REF`, SW: `SW_IN_F`, VPD: `VPD_F`, TA: `TA_F`\n",
        "- **GPP (Gross Primary Productivity)**: The rate at which plants in an ecosystem produce useful chemical energy through photosynthesis, minus the energy used for respiration. Measured in grams of carbon per square meter per day (gC m-2 day-1).\n",
        "- **SW (Soil Water Content)**: The volumetric water content of the soil. Measured in percentage (%).\n",
        "- **VPD (Vapor Pressure Deficit)**: The difference between the amount of moisture in the air and how much moisture the air can hold when it is saturated. Measured in kilopascals (kPa).\n",
        "- **TA (Air Temperature)**: The temperature of the air. Measured in degrees Celsius (°C).\n",
        "\n",
        "## Tasks\n",
        "\n",
        "### Task 1: Data Exploration and Preprocessing (2 pts)\n",
        "\n",
        "1. Load the `Wcr_GPPdaily.csv` dataset using pandas or numpy.\n",
        "2. Explore the dataset to understand its structure and contents.\n",
        "3. Check for and handle any missing or anomalous values in the data.\n",
        "4. Extract the relevant columns (GPP, SW, VPD, TA) for this analysis.\n",
        "\n",
        "### Task 2: Building the Linear Regression Model (2 pts)\n",
        "\n",
        "Define a linear regression model where GPP is the dependent variable, and SW, VPD, and TA are independent variables. The model can be represented as:\n",
        "\n",
        "\\[ GPP = \\beta_0 + \\beta_1 \\times SW + \\beta_2 \\times VPD + \\beta_3 \\times TA \\]\n",
        "\n",
        "where \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\) are the parameters to be optimized.\n",
        "\n",
        "### Task 3: Optimization with SciPy (4 pts)\n",
        "\n",
        "1. Define the objective function to minimize, which in this case could be the sum of squared residuals (the difference between observed and predicted GPP values).\n",
        "2. Use `scipy.optimize.minimize` to find the optimal values of \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\) that minimize the objective function.\n",
        "\n",
        "### Task 4: Model Evaluation (2 pts)\n",
        "\n",
        "1. Calculate the predictions of GPP using the optimized model parameters.\n",
        "2. Evaluate the model's performance by computing the R-squared value and plotting the observed vs. predicted GPP values.\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "1. A colab notebook containing all the code for data preprocessing, model building, optimization, and evaluation.\n",
        "2. Detailed comments!\n",
        "\n",
        "## Hints\n",
        "\n",
        "- Before optimization, ensure your data is scaled appropriately, as this can significantly impact the performance of the optimization algorithm.\n",
        "- Consider initializing your parameters (\\(\\beta\\)) with zeros or small random values.\n",
        "- Explore different optimization methods available in `scipy.optimize.minimize` to see which works best for your model.\n",
        "\n",
        "Good luck, and have fun exploring the power of linear regression and optimization in scientific computing!\n"
      ],
      "metadata": {
        "id": "UQUh0rDge2iH"
      }
    }
  ]
}